{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65805c18",
   "metadata": {},
   "source": [
    "# CSV Data Analysis Tutorial - Jupyter Notebook Format\n",
    "# ====================================================\n",
    "# Save this as: csv_analysis_tutorial.ipynb\n",
    "# Each section represents a separate Jupyter notebook cell\n",
    "\n",
    "# CELL 1: Title and Introduction\n",
    "# ===============================\n",
    "\"\"\"\n",
    "# üìä CSV Data Analysis Tutorial for Beginners\n",
    "\n",
    "Welcome to this comprehensive tutorial on CSV data analysis using Python and pandas!\n",
    "\n",
    "## What you'll learn:\n",
    "- Loading and exploring CSV files\n",
    "- Data quality assessment\n",
    "- Basic statistics and summaries\n",
    "- Filtering and selecting data\n",
    "- Grouping and aggregation\n",
    "- Data transformation techniques\n",
    "- Sorting and ranking\n",
    "- Exporting processed data\n",
    "\n",
    "## Prerequisites:\n",
    "Make sure you have installed: `pip install pandas numpy matplotlib seaborn faker`\n",
    "\n",
    "Let's start by importing the necessary libraries and loading our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293d32ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìö Libraries loaded:\n",
      "- pandas: Data manipulation and analysis\n",
      "- numpy: Numerical operations\n",
      "- matplotlib & seaborn: Data visualization\n",
      "- datetime: Date and time operations\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Import Libraries\n",
    "# ========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üìö Libraries loaded:\")\n",
    "print(\"- pandas: Data manipulation and analysis\")\n",
    "print(\"- numpy: Numerical operations\") \n",
    "print(\"- matplotlib & seaborn: Data visualization\")\n",
    "print(\"- datetime: Date and time operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80636d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Load and Explore Data - Function Definition\n",
    "# ===================================================\n",
    "\"\"\"\n",
    "## üìÇ Step 1: Loading and Exploring CSV Data\n",
    "\n",
    "The first step in any data analysis is to load your data and understand its structure.\n",
    "This function will help us:\n",
    "- Load the CSV file into a pandas DataFrame\n",
    "- Check the shape (rows and columns)\n",
    "- Examine data types\n",
    "- Preview the first and last few rows\n",
    "- Get general information about the dataset\n",
    "\"\"\"\n",
    "\n",
    "def load_and_explore_csv(file_path):\n",
    "    \"\"\"\n",
    "    Load CSV file and perform initial exploration\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Loaded dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìä STEP 1: LOADING AND EXPLORING DATA\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Basic information about the dataset\n",
    "    print(f\"üìà Dataset shape: {df.shape}\")\n",
    "    print(f\"üìù Number of rows: {df.shape[0]}\")\n",
    "    print(f\"üìã Number of columns: {df.shape[1]}\")\n",
    "    \n",
    "    print(\"\\nüè∑Ô∏è  Column names and data types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nüëÄ First 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nüëÄ Last 5 rows:\")\n",
    "    print(df.tail())\n",
    "    \n",
    "    print(\"\\nüìä Dataset info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760fbdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä STEP 1: LOADING AND EXPLORING DATA\n",
      "==================================================\n",
      "üìà Dataset shape: (1000, 17)\n",
      "üìù Number of rows: 1000\n",
      "üìã Number of columns: 17\n",
      "\n",
      "üè∑Ô∏è  Column names and data types:\n",
      "customer_id            int64\n",
      "first_name            object\n",
      "last_name             object\n",
      "email                 object\n",
      "phone                 object\n",
      "age                    int64\n",
      "city                  object\n",
      "country               object\n",
      "job_title             object\n",
      "salary                 int64\n",
      "registration_date     object\n",
      "last_login            object\n",
      "orders_count           int64\n",
      "total_spent          float64\n",
      "customer_segment      object\n",
      "is_active               bool\n",
      "department            object\n",
      "dtype: object\n",
      "\n",
      "üëÄ First 5 rows:\n",
      "   customer_id first_name last_name                    email  \\\n",
      "0            1   Danielle   Johnson       john21@example.net   \n",
      "1            2    Roberto   Ramirez  susanrogers@example.org   \n",
      "2            3    Melissa  Delacruz     oramirez@example.com   \n",
      "3            4     Jeremy   Johnson    richard13@example.net   \n",
      "4            5     Daniel    Burton     ycarlson@example.com   \n",
      "\n",
      "                   phone  age           city country  \\\n",
      "0  001-581-896-0013x3890   58  South Bridget   Sudan   \n",
      "1     (615)759-4078x1618   65      Lake Chad  Panama   \n",
      "2          (555)434-1928   19       Grayside  Cyprus   \n",
      "3   001-653-876-7242x388   59        Coxberg  Malawi   \n",
      "4    +1-878-448-0184x514   66      Smithview   Macao   \n",
      "\n",
      "                           job_title  salary registration_date  \\\n",
      "0              Accounting technician   44592        2025-03-16   \n",
      "1                       Estate agent   43434        2025-03-27   \n",
      "2           Journalist, broadcasting   42280        2025-05-17   \n",
      "3  Research officer, political party  121924        2023-11-14   \n",
      "4          Editor, magazine features  135620        2024-09-14   \n",
      "\n",
      "                   last_login  orders_count  total_spent customer_segment  \\\n",
      "0  2025-08-14 21:47:20.948049             1      3707.75           Silver   \n",
      "1  2025-09-08 16:11:38.181553            43      3703.34           Bronze   \n",
      "2  2025-09-02 09:33:15.770728            13      1163.30           Bronze   \n",
      "3  2025-08-11 12:45:29.645101            34      2097.60         Platinum   \n",
      "4  2025-08-13 07:27:00.852692            10      3490.70             Gold   \n",
      "\n",
      "   is_active  department  \n",
      "0       True   Marketing  \n",
      "1      False          IT  \n",
      "2       True  Operations  \n",
      "3      False          IT  \n",
      "4      False   Marketing  \n",
      "\n",
      "üëÄ Last 5 rows:\n",
      "     customer_id  first_name last_name                    email  \\\n",
      "995          996  Jacqueline    Miller      iwilson@example.net   \n",
      "996          997       Sarah     Jones  taraalvarez@example.net   \n",
      "997          998     Darlene    Wilson    natalie13@example.org   \n",
      "998          999     Cynthia  Campbell  villasteven@example.net   \n",
      "999         1000     Richard      Diaz     olivia27@example.net   \n",
      "\n",
      "                     phone  age               city  \\\n",
      "995           394-602-4702   57    North Davidside   \n",
      "996      (976)646-3913x810   29  Lake Valerieville   \n",
      "997     735-406-9626x16845   34        Gregoryfort   \n",
      "998    +1-594-288-5669x489   80   West Danielmouth   \n",
      "999  001-217-926-6256x5046   67      Lake Bradport   \n",
      "\n",
      "                              country              job_title  salary  \\\n",
      "995                            Kuwait  Hydrographic surveyor   66827   \n",
      "996                          Slovenia       Technical brewer  149292   \n",
      "997  Saint Vincent and the Grenadines  Insurance underwriter  132567   \n",
      "998                            Jersey    Production engineer   41062   \n",
      "999                              Togo          Haematologist   80222   \n",
      "\n",
      "    registration_date                  last_login  orders_count  total_spent  \\\n",
      "995        2024-04-27  2025-09-06 09:03:16.139924            17      4289.29   \n",
      "996        2025-09-08  2025-08-15 04:44:39.538638            37      3694.84   \n",
      "997        2025-06-23  2025-08-10 04:17:11.020801            44      1180.98   \n",
      "998        2024-05-20  2025-08-30 00:29:17.621358             9      2977.72   \n",
      "999        2023-11-29  2025-08-16 19:01:18.086431            15      3303.34   \n",
      "\n",
      "    customer_segment  is_active department  \n",
      "995           Bronze       True    Finance  \n",
      "996         Platinum      False  Marketing  \n",
      "997           Silver       True      Sales  \n",
      "998           Silver      False         HR  \n",
      "999           Silver       True    Finance  \n",
      "\n",
      "üìä Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   customer_id        1000 non-null   int64  \n",
      " 1   first_name         1000 non-null   object \n",
      " 2   last_name          1000 non-null   object \n",
      " 3   email              1000 non-null   object \n",
      " 4   phone              1000 non-null   object \n",
      " 5   age                1000 non-null   int64  \n",
      " 6   city               1000 non-null   object \n",
      " 7   country            1000 non-null   object \n",
      " 8   job_title          1000 non-null   object \n",
      " 9   salary             1000 non-null   int64  \n",
      " 10  registration_date  1000 non-null   object \n",
      " 11  last_login         1000 non-null   object \n",
      " 12  orders_count       1000 non-null   int64  \n",
      " 13  total_spent        1000 non-null   float64\n",
      " 14  customer_segment   1000 non-null   object \n",
      " 15  is_active          1000 non-null   bool   \n",
      " 16  department         1000 non-null   object \n",
      "dtypes: bool(1), float64(1), int64(4), object(11)\n",
      "memory usage: 126.1+ KB\n",
      "None\n",
      "\n",
      "‚úÖ Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Execute Data Loading\n",
    "# ============================\n",
    "\"\"\"\n",
    "### Now let's load our sample data:\n",
    "Make sure you have the `sample_customer_data.csv` file in your working directory.\n",
    "If you don't have it, run the CSV generator script first.\n",
    "\"\"\"\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    df = load_and_explore_csv('sample_customer_data.csv')\n",
    "    print(\"\\n‚úÖ Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå File not found! Please ensure 'sample_customer_data.csv' is in your working directory.\")\n",
    "    print(\"üí° Run the CSV generator script first to create the sample data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5b664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: Data Quality Assessment - Function Definition\n",
    "# =====================================================\n",
    "\"\"\"\n",
    "## üîç Step 2: Data Quality Assessment\n",
    "\n",
    "Before analyzing data, we need to check its quality:\n",
    "- **Missing values**: Are there any empty cells?\n",
    "- **Duplicates**: Are there any repeated records?\n",
    "- **Data types**: Are columns in the correct format?\n",
    "- **Unique values**: How many unique values does each column have?\n",
    "\n",
    "This step is crucial because poor data quality can lead to incorrect analysis results.\n",
    "\"\"\"\n",
    "\n",
    "def assess_data_quality(df):\n",
    "    \"\"\"\n",
    "    Check for missing values, duplicates, and data quality issues\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataset to assess\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Same dataset (unchanged)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìã STEP 2: DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"üîç Missing values per column:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_columns = missing_values[missing_values > 0]\n",
    "    \n",
    "    if len(missing_columns) > 0:\n",
    "        print(missing_columns)\n",
    "        print(f\"‚ö†Ô∏è  Total missing values: {missing_values.sum()}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No missing values found!\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    print(f\"\\nüîÑ Duplicate rows: {duplicate_count}\")\n",
    "    \n",
    "    if duplicate_count == 0:\n",
    "        print(\"‚úÖ No duplicate rows found!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Found {duplicate_count} duplicate rows that may need attention\")\n",
    "    \n",
    "    # Check data types and unique values\n",
    "    print(\"\\nüìä Data type analysis:\")\n",
    "    print(f\"{'Column':<20} {'Data Type':<15} {'Unique Values':<15} {'Sample Value'}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        sample_value = str(df[col].iloc[0]) if not df[col].empty else \"N/A\"\n",
    "        if len(sample_value) > 20:\n",
    "            sample_value = sample_value[:17] + \"...\"\n",
    "        print(f\"{col:<20} {str(df[col].dtype):<15} {df[col].nunique():<15} {sample_value}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d89dbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã STEP 2: DATA QUALITY ASSESSMENT\n",
      "==================================================\n",
      "üîç Missing values per column:\n",
      "‚úÖ No missing values found!\n",
      "\n",
      "üîÑ Duplicate rows: 0\n",
      "‚úÖ No duplicate rows found!\n",
      "\n",
      "üìä Data type analysis:\n",
      "Column               Data Type       Unique Values   Sample Value\n",
      "-----------------------------------------------------------------\n",
      "customer_id          int64           1000            1\n",
      "first_name           object          346             Danielle\n",
      "last_name            object          469             Johnson\n",
      "email                object          997             john21@example.net\n",
      "phone                object          1000            001-581-896-0013x...\n",
      "age                  int64           63              58\n",
      "city                 object          973             South Bridget\n",
      "country              object          238             Sudan\n",
      "job_title            object          510             Accounting techni...\n",
      "salary               int64           994             44592\n",
      "registration_date    object          544             2025-03-16\n",
      "last_login           object          1000            2025-08-14 21:47:...\n",
      "orders_count         int64           51              1\n",
      "total_spent          float64         999             3707.75\n",
      "customer_segment     object          4               Silver\n",
      "is_active            bool            2               True\n",
      "department           object          6               Marketing\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: Execute Data Quality Assessment\n",
    "# =======================================\n",
    "\"\"\"\n",
    "### Let's assess the quality of our dataset:\n",
    "\"\"\"\n",
    "\n",
    "df = assess_data_quality(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b90e69fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà STEP 3: BASIC STATISTICS AND SUMMARIES\n",
      "==================================================\n",
      "üî¢ Numerical columns summary:\n",
      "       customer_id      age     salary  orders_count  total_spent\n",
      "count      1000.00  1000.00    1000.00       1000.00      1000.00\n",
      "mean        500.50    48.44   88998.61         25.39      2536.68\n",
      "std         288.82    18.60   34844.08         14.53      1414.08\n",
      "min           1.00    18.00   30167.00          0.00         0.46\n",
      "25%         250.75    32.00   58642.75         13.75      1366.46\n",
      "50%         500.50    48.00   87106.00         25.50      2579.85\n",
      "75%         750.25    65.00  120222.50         38.00      3732.20\n",
      "max        1000.00    80.00  149946.00         50.00      4996.87\n",
      "\n",
      "üìù Categorical columns: ['first_name', 'last_name', 'email', 'phone', 'city', 'country', 'job_title', 'registration_date', 'last_login', 'customer_segment', 'is_active', 'department']\n",
      "\n",
      "üìä first_name - Value counts:\n",
      "first_name\n",
      "Michael        29\n",
      "James          18\n",
      "Christopher    17\n",
      "John           17\n",
      "David          16\n",
      "Robert         15\n",
      "Brian          14\n",
      "Jennifer       13\n",
      "Amy            11\n",
      "Jessica        11\n",
      "Name: count, dtype: int64\n",
      "... and 336 more unique values\n",
      "\n",
      "üìä last_name - Value counts:\n",
      "last_name\n",
      "Williams    21\n",
      "Johnson     21\n",
      "Smith       20\n",
      "Brown       14\n",
      "Jones       13\n",
      "Davis       12\n",
      "Martin      11\n",
      "Lee         11\n",
      "Garcia       9\n",
      "White        8\n",
      "Name: count, dtype: int64\n",
      "... and 459 more unique values\n",
      "\n",
      "üìä email - Value counts:\n",
      "email\n",
      "bjones@example.net        2\n",
      "irodriguez@example.com    2\n",
      "davidsmith@example.org    2\n",
      "jacob32@example.net       1\n",
      "jcruz@example.net         1\n",
      "ggarrett@example.com      1\n",
      "krista02@example.org      1\n",
      "alexis58@example.com      1\n",
      "hbecker@example.com       1\n",
      "rmueller@example.net      1\n",
      "Name: count, dtype: int64\n",
      "... and 987 more unique values\n",
      "\n",
      "üìä phone - Value counts:\n",
      "phone\n",
      "001-581-896-0013x3890     1\n",
      "(615)759-4078x1618        1\n",
      "(555)434-1928             1\n",
      "001-653-876-7242x388      1\n",
      "+1-878-448-0184x514       1\n",
      "001-380-395-7015x43039    1\n",
      "001-734-265-7871x331      1\n",
      "(399)873-7631             1\n",
      "387.926.2473              1\n",
      "(960)464-7468             1\n",
      "Name: count, dtype: int64\n",
      "... and 990 more unique values\n",
      "\n",
      "üìä city - Value counts:\n",
      "city\n",
      "New Bradley         2\n",
      "Wilsonbury          2\n",
      "Andersonburgh       2\n",
      "South Erin          2\n",
      "Amyberg             2\n",
      "Port Jessica        2\n",
      "Lake Daniel         2\n",
      "West Christopher    2\n",
      "Matthewview         2\n",
      "Thompsonhaven       2\n",
      "Name: count, dtype: int64\n",
      "... and 963 more unique values\n",
      "\n",
      "üìä country - Value counts:\n",
      "country\n",
      "Poland                              12\n",
      "Saint Vincent and the Grenadines    10\n",
      "Saudi Arabia                        10\n",
      "Slovenia                             9\n",
      "Saint Martin                         9\n",
      "Sri Lanka                            9\n",
      "Somalia                              9\n",
      "French Southern Territories          9\n",
      "Egypt                                8\n",
      "Korea                                8\n",
      "Name: count, dtype: int64\n",
      "... and 228 more unique values\n",
      "\n",
      "üìä job_title - Value counts:\n",
      "job_title\n",
      "Bonds trader                                     7\n",
      "Chartered legal executive (England and Wales)    6\n",
      "Company secretary                                6\n",
      "Scientist, physiological                         5\n",
      "Exhibitions officer, museum/gallery              5\n",
      "Passenger transport manager                      5\n",
      "Teacher, adult education                         5\n",
      "Sports coach                                     5\n",
      "Administrator, local government                  5\n",
      "Buyer, retail                                    5\n",
      "Name: count, dtype: int64\n",
      "... and 500 more unique values\n",
      "\n",
      "üìä registration_date - Value counts:\n",
      "registration_date\n",
      "2024-04-04    5\n",
      "2023-11-29    5\n",
      "2024-10-15    5\n",
      "2024-10-12    5\n",
      "2024-12-31    5\n",
      "2024-04-09    5\n",
      "2025-05-18    5\n",
      "2024-01-24    5\n",
      "2025-04-09    4\n",
      "2024-07-02    4\n",
      "Name: count, dtype: int64\n",
      "... and 534 more unique values\n",
      "\n",
      "üìä last_login - Value counts:\n",
      "last_login\n",
      "2025-08-14 21:47:20.948049    1\n",
      "2025-09-08 16:11:38.181553    1\n",
      "2025-09-02 09:33:15.770728    1\n",
      "2025-08-11 12:45:29.645101    1\n",
      "2025-08-13 07:27:00.852692    1\n",
      "2025-08-24 09:02:15.924032    1\n",
      "2025-09-06 06:44:12.673712    1\n",
      "2025-09-08 15:26:00.227861    1\n",
      "2025-09-06 21:49:07.219957    1\n",
      "2025-08-11 22:43:26.025267    1\n",
      "Name: count, dtype: int64\n",
      "... and 990 more unique values\n",
      "\n",
      "üìä customer_segment - Value counts:\n",
      "customer_segment\n",
      "Bronze      254\n",
      "Silver      253\n",
      "Platinum    252\n",
      "Gold        241\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä is_active - Value counts:\n",
      "is_active\n",
      "False    504\n",
      "True     496\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä department - Value counts:\n",
      "department\n",
      "Operations    188\n",
      "Finance       178\n",
      "HR            168\n",
      "Marketing     160\n",
      "Sales         157\n",
      "IT            149\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìã Summary:\n",
      "- Numerical columns: 5\n",
      "- Categorical columns: 12\n",
      "- Total columns: 17\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Basic Statistics - Function Definition\n",
    "# ==============================================\n",
    "\"\"\"\n",
    "## üìà Step 3: Basic Statistics and Summaries\n",
    "\n",
    "Understanding the distribution and characteristics of your data is essential:\n",
    "- **Numerical data**: Mean, median, standard deviation, min/max values\n",
    "- **Categorical data**: Most common values, unique categories\n",
    "- **Data distribution**: Understanding how values are spread\n",
    "\n",
    "This helps identify patterns, outliers, and potential issues in your data.\n",
    "\"\"\"\n",
    "\n",
    "def generate_basic_statistics(df):\n",
    "    \"\"\"\n",
    "    Generate descriptive statistics for the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataset to analyze\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (numerical_columns, categorical_columns)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìà STEP 3: BASIC STATISTICS AND SUMMARIES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'bool']).columns\n",
    "    \n",
    "    # Numerical columns statistics\n",
    "    if len(numerical_cols) > 0:\n",
    "        print(\"üî¢ Numerical columns summary:\")\n",
    "        print(df[numerical_cols].describe().round(2))\n",
    "    \n",
    "    # Categorical columns analysis\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\nüìù Categorical columns: {list(categorical_cols)}\")\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            print(f\"\\nüìä {col} - Value counts:\")\n",
    "            value_counts = df[col].value_counts().head(10)  # Show top 10 values\n",
    "            print(value_counts)\n",
    "            \n",
    "            if len(df[col].value_counts()) > 10:\n",
    "                print(f\"... and {len(df[col].value_counts()) - 10} more unique values\")\n",
    "    \n",
    "    print(f\"\\nüìã Summary:\")\n",
    "    print(f\"- Numerical columns: {len(numerical_cols)}\")\n",
    "    print(f\"- Categorical columns: {len(categorical_cols)}\")\n",
    "    print(f\"- Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    return numerical_cols, categorical_cols\n",
    "\n",
    "# CELL 8: Execute Basic Statistics\n",
    "# ================================\n",
    "\"\"\"\n",
    "### Let's analyze the statistical properties of our data:\n",
    "\"\"\"\n",
    "\n",
    "numerical_cols, categorical_cols = generate_basic_statistics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5f304",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (986894502.py, line 746)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 746\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# CELL 9: Data Filtering - Function Definition\n",
    "# ============================================\n",
    "\"\"\"\n",
    "## üîç Step 4: Data Filtering and Selection\n",
    "\n",
    "Filtering is one of the most important skills in data analysis:\n",
    "- **Column selection**: Choose specific columns for analysis\n",
    "- **Row filtering**: Find records that meet certain conditions\n",
    "- **Multiple conditions**: Combine filters using AND (&) and OR (|) operators\n",
    "- **Text filtering**: Search for specific text patterns\n",
    "\n",
    "This allows you to focus on relevant subsets of your data.\n",
    "\"\"\"\n",
    "\n",
    "def demonstrate_filtering(df):\n",
    "    \"\"\"\n",
    "    Show various ways to filter and select data\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataset to filter\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (filtered_dataframes...)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîç STEP 4: DATA FILTERING AND SELECTION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Select specific columns\n",
    "    print(\"üìã 1. Selecting specific columns (name and age):\")\n",
    "    basic_info = df[['first_name', 'last_name', 'age']].head()\n",
    "    print(basic_info)\n",
    "    print(f\"Selected {len(basic_info.columns)} columns from original {len(df.columns)}\")\n",
    "    \n",
    "    # 2. Filter rows based on single condition\n",
    "    print(\"\\nüîç 2. Filtering: Customers older than 50:\")\n",
    "    older_customers = df[df['age'] > 50]\n",
    "    print(f\"Found {len(older_customers)} customers older than 50 (out of {len(df)} total)\")\n",
    "    print(older_customers[['first_name', 'last_name', 'age']].head())\n",
    "    \n",
    "    # 3. Multiple conditions with AND\n",
    "    print(\"\\nüí∞ 3. Filtering: Active customers with high spending (>$2000):\")\n",
    "    high_value_active = df[(df['is_active'] == True) & (df['total_spent'] > 2000)]\n",
    "    print(f\"Found {len(high_value_active)} high-value active customers\")\n",
    "    if len(high_value_active) > 0:\n",
    "        print(high_value_active[['first_name', 'last_name', 'total_spent', 'is_active']].head())\n",
    "    \n",
    "    # 4. Filter by categorical values\n",
    "    print(\"\\nüíº 4. Filtering: Customers from IT department:\")\n",
    "    it_customers = df[df['department'] == 'IT']\n",
    "    print(f\"Found {len(it_customers)} IT department customers\")\n",
    "    if len(it_customers) > 0:\n",
    "        print(it_customers[['first_name', 'last_name', 'department', 'job_title']].head())\n",
    "    \n",
    "    # 5. Filter using isin() for multiple values\n",
    "    print(\"\\nüèÜ 5. Filtering: Premium customers (Gold or Platinum):\")\n",
    "    premium_customers = df[df['customer_segment'].isin(['Gold', 'Platinum'])]\n",
    "    print(f\"Found {len(premium_customers)} premium customers\")\n",
    "    print(premium_customers['customer_segment'].value_counts())\n",
    "    \n",
    "    # 6. String filtering (contains)\n",
    "    print(\"\\nüìß 6. Filtering: Customers with gmail addresses:\")\n",
    "    gmail_customers = df[df['email'].str.contains('@gmail.com', na=False)]\n",
    "    print(f\"Found {len(gmail_customers)} customers with Gmail addresses\")\n",
    "    \n",
    "    return older_customers, high_value_active, it_customers, premium_customers\n",
    "\n",
    "# CELL 10: Execute Data Filtering\n",
    "# ===============================\n",
    "\"\"\"\n",
    "### Let's explore different filtering techniques:\n",
    "\"\"\"\n",
    "\n",
    "older_customers, high_value_active, it_customers, premium_customers = demonstrate_filtering(df)\n",
    "\n",
    "# CELL 11: Data Grouping and Aggregation - Function Definition\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "## üìä Step 5: Data Grouping and Aggregation\n",
    "\n",
    "Grouping allows you to analyze data by categories:\n",
    "- **Single grouping**: Group by one column (e.g., by department)\n",
    "- **Multiple grouping**: Group by several columns (e.g., by department and segment)\n",
    "- **Aggregation functions**: Calculate statistics for each group (mean, sum, count, etc.)\n",
    "- **Multiple aggregations**: Calculate different statistics for different columns\n",
    "\n",
    "This is essential for creating summaries and understanding patterns across different categories.\n",
    "\"\"\"\n",
    "\n",
    "def demonstrate_grouping(df):\n",
    "    \"\"\"\n",
    "    Show how to group data and perform aggregations\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataset to group\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (grouped_results...)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìä STEP 5: DATA GROUPING AND AGGREGATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Group by single column - simple aggregation\n",
    "    print(\"üí∞ 1. Average spending by customer segment:\")\n",
    "    segment_spending = df.groupby('customer_segment')['total_spent'].mean().round(2)\n",
    "    print(segment_spending.sort_values(ascending=False))\n",
    "    \n",
    "    # 2. Group by single column - multiple statistics\n",
    "    print(\"\\nüìä 2. Detailed statistics by customer segment:\")\n",
    "    segment_stats = df.groupby('customer_segment')['total_spent'].agg(['count', 'mean', 'median', 'std']).round(2)\n",
    "    segment_stats.columns = ['Customer Count', 'Avg Spending', 'Median Spending', 'Std Deviation']\n",
    "    print(segment_stats)\n",
    "    \n",
    "    # 3. Group by multiple columns\n",
    "    print(\"\\nüè¢ 3. Average salary by department and customer segment:\")\n",
    "    dept_segment_salary = df.groupby(['department', 'customer_segment'])['salary'].mean().round(2)\n",
    "    print(dept_segment_salary.head(15))\n",
    "    \n",
    "    # 4. Multiple aggregations on different columns\n",
    "    print(\"\\nüìà 4. Comprehensive statistics by department:\")\n",
    "    dept_stats = df.groupby('department').agg({\n",
    "        'salary': ['mean', 'median', 'min', 'max'],\n",
    "        'age': 'mean',\n",
    "        'total_spent': ['sum', 'mean'],\n",
    "        'customer_id': 'count',\n",
    "        'is_active': lambda x: (x == True).sum()  # Count of active customers\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names for better readability\n",
    "    dept_stats.columns = ['Avg_Salary', 'Med_Salary', 'Min_Salary', 'Max_Salary', \n",
    "                         'Avg_Age', 'Total_Revenue', 'Avg_Spending', 'Customer_Count', 'Active_Count']\n",
    "    print(dept_stats)\n",
    "    \n",
    "    # 5. Percentage calculations\n",
    "    print(\"\\nüìä 5. Active customer percentage by segment:\")\n",
    "    active_pct = df.groupby('customer_segment').apply(\n",
    "        lambda x: (x['is_active'].sum() / len(x) * 100).round(1)\n",
    "    ).sort_values(ascending=False)\n",
    "    print(active_pct)\n",
    "    \n",
    "    return segment_spending, dept_stats, active_pct\n",
    "\n",
    "# CELL 12: Execute Grouping and Aggregation\n",
    "# =========================================\n",
    "\"\"\"\n",
    "### Let's group our data and calculate meaningful statistics:\n",
    "\"\"\"\n",
    "\n",
    "segment_spending, dept_stats, active_pct = demonstrate_grouping(df)\n",
    "\n",
    "# CELL 13: Data Transformation - Function Definition\n",
    "# ==================================================\n",
    "\"\"\"\n",
    "## üîÑ Step 6: Data Transformation and Feature Engineering\n",
    "\n",
    "Creating new columns and transforming existing data:\n",
    "- **Calculated columns**: Create new columns from existing ones\n",
    "- **Categorical binning**: Convert continuous variables into categories\n",
    "- **Text manipulation**: Combine or split text columns\n",
    "- **Conditional columns**: Create columns based on conditions\n",
    "- **Mathematical operations**: Apply formulas to create derived metrics\n",
    "\n",
    "This helps create more meaningful insights and prepare data for analysis.\n",
    "\"\"\"\n",
    "\n",
    "def demonstrate_transformation(df):\n",
    "    \"\"\"\n",
    "    Show how to create new columns and transform data\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataset to transform\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Transformed dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîÑ STEP 6: DATA TRANSFORMATION AND FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a copy to avoid modifying original data\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    # 1. Simple column combination\n",
    "    print(\"üë• 1. Creating full name from first and last name:\")\n",
    "    df_transformed['full_name'] = df_transformed['first_name'] + ' ' + df_transformed['last_name']\n",
    "    print(\"‚úÖ Created 'full_name' column\")\n",
    "    print(df_transformed['full_name'].head())\n",
    "    \n",
    "    # 2. Mathematical calculations\n",
    "    print(\"\\nüí∞ 2. Calculating spending per order:\")\n",
    "    # Add 1 to avoid division by zero\n",
    "    df_transformed['spending_per_order'] = (df_transformed['total_spent'] / \n",
    "                                          (df_transformed['orders_count'] + 1)).round(2)\n",
    "    print(\"‚úÖ Created 'spending_per_order' column\")\n",
    "    print(f\"Average spending per order: ${df_transformed['spending_per_order'].mean():.2f}\")\n",
    "    \n",
    "    # 3. Categorical binning - Age categories\n",
    "    print(\"\\nüë¥ 3. Creating age categories:\")\n",
    "    def categorize_age(age):\n",
    "        if age < 30:\n",
    "            return 'Young (18-29)'\n",
    "        elif age < 50:\n",
    "            return 'Middle-aged (30-49)'\n",
    "        else:\n",
    "            return 'Senior (50+)'\n",
    "    \n",
    "    df_transformed['age_category'] = df_transformed['age'].apply(categorize_age)\n",
    "    print(\"‚úÖ Created 'age_category' column\")\n",
    "    print(df_transformed['age_category'].value_counts())\n",
    "    \n",
    "    # 4. Binary indicator columns\n",
    "    print(\"\\nüíé 4. Creating binary indicator columns:\")\n",
    "    median_spending = df_transformed['total_spent'].median()\n",
    "    df_transformed['high_spender'] = df_transformed['total_spent'] > median_spending\n",
    "    df_transformed['high_earner'] = df_transformed['salary'] > df_transformed['salary'].median()\n",
    "    print(f\"‚úÖ Created binary columns based on median values\")\n",
    "    print(f\"High spenders (>${median_spending:.2f}+): {df_transformed['high_spender'].sum()}\")\n",
    "    \n",
    "    # 5. Date calculations (if date columns exist)\n",
    "    if 'registration_date' in df_transformed.columns:\n",
    "        print(\"\\nüìÖ 5. Working with dates:\")\n",
    "        df_transformed['registration_date'] = pd.to_datetime(df_transformed['registration_date'])\n",
    "        df_transformed['days_since_registration'] = (pd.Timestamp.now() - df_transformed['registration_date']).dt.days\n",
    "        print(\"‚úÖ Created 'days_since_registration' column\")\n",
    "        print(f\"Average customer tenure: {df_transformed['days_since_registration'].mean():.0f} days\")\n",
    "    \n",
    "    # 6. Text manipulation\n",
    "    print(\"\\nüìß 6. Email domain extraction:\")\n",
    "    df_transformed['email_domain'] = df_transformed['email'].str.split('@').str[1]\n",
    "    print(\"‚úÖ Created 'email_domain' column\")\n",
    "    print(\"Top email domains:\")\n",
    "    print(df_transformed['email_domain'].value_counts().head())\n",
    "    \n",
    "    # Display summary of new columns\n",
    "    new_columns = ['full_name', 'spending_per_order', 'age_category', 'high_spender', 'email_domain']\n",
    "    if 'days_since_registration' in df_transformed.columns:\n",
    "        new_columns.append('days_since_registration')\n",
    "    \n",
    "    print(f\"\\nüìã Summary: Created {len(new_columns)} new columns:\")\n",
    "    for col in new_columns:\n",
    "        if col in df_transformed.columns:\n",
    "            print(f\"- {col}\")\n",
    "    \n",
    "    print(f\"\\nDataset now has {len(df_transformed.columns)} columns (was {len(df.columns)})\")\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "# CELL 14: Execute Data Transformation\n",
    "# ===================================\n",
    "\"\"\"\n",
    "### Let's create new meaningful columns from our existing data:\n",
    "\"\"\"\n",
    "\n",
    "df_transformed = demonstrate_transformation(df)\n",
    "\n",
    "# CELL 15: Preview Transformed Data\n",
    "# ================================\n",
    "\"\"\"\n",
    "### Let's see our transformed data with new columns:\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç TRANSFORMED DATA PREVIEW\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Show some of the new columns\n",
    "preview_columns = ['full_name', 'age', 'age_category', 'total_spent', 'spending_per_order', 'high_spender']\n",
    "available_columns = [col for col in preview_columns if col in df_transformed.columns]\n",
    "\n",
    "print(\"Sample of transformed data:\")\n",
    "print(df_transformed[available_columns].head(10))\n",
    "\n",
    "# CELL 16: Sorting and Ranking - Function Definition\n",
    "# ==================================================\n",
    "\"\"\"\n",
    "## üìã Step 7: Sorting and Ranking Data\n",
    "\n",
    "Organizing data in meaningful order:\n",
    "- **Single column sorting**: Order by one criterion\n",
    "- **Multiple column sorting**: Order by multiple criteria with different priorities\n",
    "- **Top/Bottom N**: Find the highest or lowest values\n",
    "- **Ranking**: Assign ranks to data points\n",
    "\n",
    "This helps identify patterns, outliers, and top performers in your data.\n",
    "\"\"\"\n",
    "\n",
    "def demonstrate_sorting(df):\n",
    "    \"\"\"\n",
    "    Show different ways to sort data\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataset to sort\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (sorted_results...)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìã STEP 7: SORTING AND RANKING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Sort by single column - highest values\n",
    "    print(\"üí∞ 1. Top 10 highest spenders:\")\n",
    "    top_spenders = df.nlargest(10, 'total_spent')[['first_name', 'last_name', 'total_spent', 'customer_segment']]\n",
    "    print(top_spenders)\n",
    "    \n",
    "    # 2. Sort by single column - lowest values\n",
    "    print(\"\\nüí∏ 2. Bottom 5 spenders:\")\n",
    "    low_spenders = df.nsmallest(5, 'total_spent')[['first_name', 'last_name', 'total_spent']]\n",
    "    print(low_spenders)\n",
    "    \n",
    "    # 3. Sort by multiple columns\n",
    "    print(\"\\nüè¢ 3. Sorted by department, then by salary (descending):\")\n",
    "    sorted_data = df.sort_values(['department', 'salary'], ascending=[True, False])\n",
    "    display_cols = ['first_name', 'last_name', 'department', 'salary', 'customer_segment']\n",
    "    print(sorted_data[display_cols].head(15))\n",
    "    \n",
    "    # 4. Add ranking columns\n",
    "    print(\"\\nüèÜ 4. Creating ranking columns:\")\n",
    "    df_ranked = df.copy()\n",
    "    df_ranked['spending_rank'] = df_ranked['total_spent'].rank(ascending=False, method='dense')\n",
    "    df_ranked['salary_rank'] = df_ranked['salary'].rank(ascending=False, method='dense')\n",
    "    \n",
    "    # Show top 10 by spending rank\n",
    "    top_ranked = df_ranked.nsmallest(10, 'spending_rank')[\n",
    "        ['first_name', 'last_name', 'total_spent', 'spending_rank', 'customer_segment']\n",
    "    ]\n",
    "    print(top_ranked)\n",
    "    \n",
    "    # 5. Sort by custom criteria\n",
    "    print(\"\\n‚≠ê 5. Best customers (high spending + high orders):\")\n",
    "    df['customer_score'] = (df['total_spent'] * 0.7) + (df['orders_count'] * 100 * 0.3)\n",
    "    best_customers = df.nlargest(10, 'customer_score')[\n",
    "        ['first_name', 'last_name', 'total_spent', 'orders_count', 'customer_score']\n",
    "    ]\n",
    "    print(best_customers.round(2))\n",
    "    \n",
    "    return top_spenders, best_customers\n",
    "\n",
    "# CELL 17: Execute Sorting and Ranking\n",
    "# ====================================\n",
    "\"\"\"\n",
    "### Let's sort our data to find patterns and top performers:\n",
    "\"\"\"\n",
    "\n",
    "top_spenders, best_customers = demonstrate_sorting(df_transformed)\n",
    "\n",
    "# CELL 18: Data Export - Function Definition\n",
    "# ==========================================\n",
    "\"\"\"\n",
    "## üíæ Step 8: Saving and Exporting Data\n",
    "\n",
    "After analyzing and transforming data, you'll want to save your results:\n",
    "- **Full dataset**: Save the complete transformed dataset\n",
    "- **Filtered subsets**: Save specific filtered data\n",
    "- **Summary reports**: Export aggregated results\n",
    "- **Different formats**: CSV, Excel, or other formats\n",
    "\n",
    "This preserves your work and allows sharing results with others.\n",
    "\"\"\"\n",
    "\n",
    "def save_processed_data(df, df_transformed, filename_prefix='processed'):\n",
    "    \"\"\"\n",
    "    Save processed data to new CSV files\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Original dataset\n",
    "    df_transformed (pandas.DataFrame): Transformed dataset\n",
    "    filename_prefix (str): Prefix for output files\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üíæ STEP 8: SAVING AND EXPORTING DATA\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Save the complete transformed dataset\n",
    "    transformed_filename = f'{filename_prefix}_complete_data.csv'\n",
    "    df_transformed.to_csv(transformed_filename, index=False)\n",
    "    print(f\"‚úÖ Complete transformed data saved to: {transformed_filename}\")\n",
    "    print(f\"   - Rows: {len(df_transformed)}, Columns: {len(df_transformed.columns)}\")\n",
    "    \n",
    "    # 2. Save a customer summary report\n",
    "    summary_filename = f'{filename_prefix}_customer_summary.csv'\n",
    "    summary_columns = ['customer_id', 'full_name', 'age', 'age_category', \n",
    "                      'total_spent', 'customer_segment', 'department', 'is_active']\n",
    "    available_summary_cols = [col for col in summary_columns if col in df_transformed.columns]\n",
    "    \n",
    "    df_transformed[available_summary_cols].to_csv(summary_filename, index=False)\n",
    "    print(f\"‚úÖ Customer summary saved to: {summary_filename}\")\n",
    "    print(f\"   - Rows: {len(df_transformed)}, Columns: {len(available_summary_cols)}\")\n",
    "    \n",
    "    # 3. Save high-value customers only\n",
    "    if 'high_spender' in df_transformed.columns:\n",
    "        high_value_filename = f'{filename_prefix}_high_value_customers.csv'\n",
    "        high_value_customers = df_transformed[df_transformed['high_spender'] == True]\n",
    "        high_value_customers.to_csv(high_value_filename, index=False)\n",
    "        print(f\"‚úÖ High-value customers saved to: {high_value_filename}\")\n",
    "        print(f\"   - Rows: {len(high_value_customers)}, Columns: {len(high_value_customers.columns)}\")\n",
    "    \n",
    "    # 4. Save department analysis\n",
    "    dept_analysis_filename = f'{filename_prefix}_department_analysis.csv'\n",
    "    dept_analysis = df_transformed.groupby('department').agg({\n",
    "        'customer_id': 'count',\n",
    "        'salary': 'mean',\n",
    "        'total_spent': 'mean',\n",
    "        'age': 'mean',\n",
    "        'is_active': lambda x: (x == True).sum()\n",
    "    }).round(2)\n",
    "    dept_analysis.columns = ['Employee_Count', 'Avg_Salary', 'Avg_Spending', 'Avg_Age', 'Active_Count']\n",
    "    dept_analysis.to_csv(dept_analysis_filename)\n",
    "    print(f\"‚úÖ Department analysis saved to: {dept_analysis_filename}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ All files saved successfully!\")\n",
    "    return transformed_filename, summary_filename\n",
    "\n",
    "# CELL 19: Execute Data Export\n",
    "# ============================\n",
    "\"\"\"\n",
    "### Let's save our processed data and analysis results:\n",
    "\"\"\"\n",
    "\n",
    "transformed_file, summary_file = save_processed_data(df, df_transformed)\n",
    "\n",
    "# CELL 20: Advanced Analysis Examples\n",
    "# ===================================\n",
    "\"\"\"\n",
    "## üéì Step 9: Advanced Analysis Examples\n",
    "\n",
    "Now that you've learned the basics, let's explore some advanced techniques:\n",
    "\"\"\"\n",
    "\n",
    "print(\"üéì ADVANCED ANALYSIS EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Correlation Analysis\n",
    "print(\"üìä 1. Correlation analysis between numerical variables:\")\n",
    "numerical_data = df_transformed.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numerical_data.corr().round(3)\n",
    "\n",
    "# Show correlations with total_spent\n",
    "if 'total_spent' in correlation_matrix.columns:\n",
    "    spending_correlations = correlation_matrix['total_spent'].sort_values(ascending=False)\n",
    "    print(\"Variables correlated with total spending:\")\n",
    "    print(spending_correlations)\n",
    "\n",
    "# 2. Cross-tabulation\n",
    "print(\"\\nüìã 2. Cross-tabulation: Customer segment vs Department:\")\n",
    "if 'age_category' in df_transformed.columns:\n",
    "    cross_tab = pd.crosstab(df_transformed['customer_segment'], \n",
    "                           df_transformed['age_category'], \n",
    "                           margins=True)\n",
    "    print(cross_tab)\n",
    "\n",
    "# 3. Percentile analysis\n",
    "print(\"\\nüìà 3. Spending percentiles by customer segment:\")\n",
    "spending_percentiles = df_transformed.groupby('customer_segment')['total_spent'].quantile([0.25, 0.5, 0.75]).round(2)\n",
    "print(spending_percentiles)\n",
    "\n",
    "# CELL 21: Data Visualization Examples\n",
    "# ====================================\n",
    "\"\"\"\n",
    "## üìä Step 10: Basic Data Visualization\n",
    "\n",
    "Let's create some simple visualizations to better understand our data:\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìä CREATING DATA VISUALIZATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Distribution of total spending\n",
    "axes[0, 0].hist(df_transformed['total_spent'], bins=30, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribution of Total Spending')\n",
    "axes[0, 0].set_xlabel('Total Spent ($)')\n",
    "axes[0, 0].set_ylabel('Number of Customers')\n",
    "\n",
    "# 2. Average spending by customer segment\n",
    "segment_avg = df_transformed.groupby('customer_segment')['total_spent'].mean()\n",
    "axes[0, 1].bar(segment_avg.index, segment_avg.values, color=['bronze', 'silver', 'gold', 'purple'])\n",
    "axes[0, 1].set_title('Average Spending by Customer Segment')\n",
    "axes[0, 1].set_ylabel('Average Spending ($)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Age distribution\n",
    "axes[1, 0].hist(df_transformed['age'], bins=20, alpha=0.7, color='lightgreen')\n",
    "axes[1, 0].set_title('Age Distribution')\n",
    "axes[1, 0].set_xlabel('Age')\n",
    "axes[1, 0].set_ylabel('Number of Customers')\n",
    "\n",
    "# 4. Customer count by department\n",
    "dept_counts = df_transformed['department'].value_counts()\n",
    "axes[1, 1].pie(dept_counts.values, labels=dept_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 1].set_title('Customer Distribution by Department')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations created successfully!\")\n",
    "\n",
    "# CELL 22: Summary and Next Steps\n",
    "# ===============================\n",
    "\"\"\"\n",
    "# üéâ Congratulations! Tutorial Complete!\n",
    "\n",
    "## What you've accomplished:\n",
    "‚úÖ **Data Loading**: Loaded and explored CSV data  \n",
    "‚úÖ **Quality Assessment**: Checked for missing values and duplicates  \n",
    "‚úÖ **Statistical Analysis**: Generated descriptive statistics  \n",
    "‚úÖ **Data Filtering**: Selected and filtered data based on conditions  \n",
    "‚úÖ **Grouping & Aggregation**: Summarized data by categories  \n",
    "‚úÖ **Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc06e20d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

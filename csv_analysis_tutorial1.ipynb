{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65805c18",
   "metadata": {},
   "source": [
    "# CSV Data Analysis Tutorial - Jupyter Notebook Format\n",
    "# ====================================================\n",
    "# Save this as: csv_analysis_tutorial.ipynb\n",
    "# Each section represents a separate Jupyter notebook cell\n",
    "\n",
    "# CELL 1: Title and Introduction\n",
    "# ===============================\n",
    "\"\"\"\n",
    "# 📊 CSV Data Analysis Tutorial for Beginners\n",
    "\n",
    "Welcome to this comprehensive tutorial on CSV data analysis using Python and pandas!\n",
    "\n",
    "## What you'll learn:\n",
    "- Loading and exploring CSV files\n",
    "- Data quality assessment\n",
    "- Basic statistics and summaries\n",
    "- Filtering and selecting data\n",
    "- Grouping and aggregation\n",
    "- Data transformation techniques\n",
    "- Sorting and ranking\n",
    "- Exporting processed data\n",
    "\n",
    "## Prerequisites:\n",
    "Make sure you have installed: `pip install pandas numpy matplotlib seaborn faker`\n",
    "\n",
    "Let's start by importing the necessary libraries and loading our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293d32ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n",
      "📚 Libraries loaded:\n",
      "- pandas: Data manipulation and analysis\n",
      "- numpy: Numerical operations\n",
      "- matplotlib & seaborn: Data visualization\n",
      "- datetime: Date and time operations\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Import Libraries\n",
    "# ========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(\"📚 Libraries loaded:\")\n",
    "print(\"- pandas: Data manipulation and analysis\")\n",
    "print(\"- numpy: Numerical operations\") \n",
    "print(\"- matplotlib & seaborn: Data visualization\")\n",
    "print(\"- datetime: Date and time operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80636d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Load and Explore Data - Function Definition\n",
    "# ===================================================\n",
    "\"\"\"\n",
    "## 📂 Step 1: Loading and Exploring CSV Data\n",
    "\n",
    "The first step in any data analysis is to load your data and understand its structure.\n",
    "This function will help us:\n",
    "- Load the CSV file into a pandas DataFrame\n",
    "- Check the shape (rows and columns)\n",
    "- Examine data types\n",
    "- Preview the first and last few rows\n",
    "- Get general information about the dataset\n",
    "\"\"\"\n",
    "\n",
    "def load_and_explore_csv(file_path):\n",
    "    \"\"\"\n",
    "    Load CSV file and perform initial exploration\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Loaded dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"📊 STEP 1: LOADING AND EXPLORING DATA\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Basic information about the dataset\n",
    "    print(f\"📈 Dataset shape: {df.shape}\")\n",
    "    print(f\"📝 Number of rows: {df.shape[0]}\")\n",
    "    print(f\"📋 Number of columns: {df.shape[1]}\")\n",
    "    \n",
    "    print(\"\\n🏷️  Column names and data types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\n👀 First 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\n👀 Last 5 rows:\")\n",
    "    print(df.tail())\n",
    "    \n",
    "    print(\"\\n📊 Dataset info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760fbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Execute Data Loading\n",
    "# ============================\n",
    "\"\"\"\n",
    "### Now let's load our sample data:\n",
    "Make sure you have the `sample_customer_data.csv` file in your working directory.\n",
    "If you don't have it, run the CSV generator script first.\n",
    "\"\"\"\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    df = load_and_explore_csv('sample_customer_data.csv')\n",
    "    print(\"\\n✅ Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ File not found! Please ensure 'sample_customer_data.csv' is in your working directory.\")\n",
    "    print(\"💡 Run the CSV generator script first to create the sample data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef5b664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 STEP 2: DATA QUALITY ASSESSMENT\n",
      "==================================================\n",
      "🔍 Missing values per column:\n",
      "✅ No missing values found!\n",
      "\n",
      "🔄 Duplicate rows: 0\n",
      "✅ No duplicate rows found!\n",
      "\n",
      "📊 Data type analysis:\n",
      "Column               Data Type       Unique Values   Sample Value\n",
      "-----------------------------------------------------------------\n",
      "customer_id          int64           1000            1\n",
      "first_name           object          346             Danielle\n",
      "last_name            object          469             Johnson\n",
      "email                object          997             john21@example.net\n",
      "phone                object          1000            001-581-896-0013x...\n",
      "age                  int64           63              58\n",
      "city                 object          973             South Bridget\n",
      "country              object          238             Sudan\n",
      "job_title            object          510             Accounting techni...\n",
      "salary               int64           994             44592\n",
      "registration_date    object          544             2025-03-16\n",
      "last_login           object          1000            2025-08-14 21:47:...\n",
      "orders_count         int64           51              1\n",
      "total_spent          float64         999             3707.75\n",
      "customer_segment     object          4               Silver\n",
      "is_active            bool            2               True\n",
      "department           object          6               Marketing\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Data Quality Assessment - Function Definition\n",
    "# =====================================================\n",
    "\"\"\"\n",
    "## 🔍 Step 2: Data Quality Assessment\n",
    "\n",
    "Before analyzing data, we need to check its quality:\n",
    "- **Missing values**: Are there any empty cells?\n",
    "- **Duplicates**: Are there any repeated records?\n",
    "- **Data types**: Are columns in the correct format?\n",
    "- **Unique values**: How many unique values does each column have?\n",
    "\n",
    "This step is crucial because poor data quality can lead to incorrect analysis results.\n",
    "\"\"\"\n",
    "\n",
    "def assess_data_quality(df):\n",
    "    \"\"\"\n",
    "    Check for missing values, duplicates, and data quality issues\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataset to assess\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Same dataset (unchanged)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"📋 STEP 2: DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"🔍 Missing values per column:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_columns = missing_values[missing_values > 0]\n",
    "    \n",
    "    if len(missing_columns) > 0:\n",
    "        print(missing_columns)\n",
    "        print(f\"⚠️  Total missing values: {missing_values.sum()}\")\n",
    "    else:\n",
    "        print(\"✅ No missing values found!\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    print(f\"\\n🔄 Duplicate rows: {duplicate_count}\")\n",
    "    \n",
    "    if duplicate_count == 0:\n",
    "        print(\"✅ No duplicate rows found!\")\n",
    "    else:\n",
    "        print(f\"⚠️  Found {duplicate_count} duplicate rows that may need attention\")\n",
    "    \n",
    "    # Check data types and unique values\n",
    "    print(\"\\n📊 Data type analysis:\")\n",
    "    print(f\"{'Column':<20} {'Data Type':<15} {'Unique Values':<15} {'Sample Value'}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        sample_value = str(df[col].iloc[0]) if not df[col].empty else \"N/A\"\n",
    "        if len(sample_value) > 20:\n",
    "            sample_value = sample_value[:17] + \"...\"\n",
    "        print(f\"{col:<20} {str(df[col].dtype):<15} {df[col].nunique():<15} {sample_value}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# CELL 6: Execute Data Quality Assessment\n",
    "# =======================================\n",
    "\"\"\"\n",
    "### Let's assess the quality of our dataset:\n",
    "\"\"\"\n",
    "\n",
    "df = assess_data_quality(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Basic Statistics - Function Definition\n",
    "# ==============================================\n",
    "\"\"\"\n",
    "## 📈 Step 3: Basic Statistics and Summaries\n",
    "\n",
    "Understanding the distribution and characteristics of your data is essential:\n",
    "- **Numerical data**: Mean, median, standard deviation, min/max values\n",
    "- **Categorical data**: Most common values, unique categories\n",
    "- **Data distribution**: Understanding how values are spread\n",
    "\n",
    "This helps identify patterns, outliers, and potential issues in your data.\n",
    "\"\"\"\n",
    "\n",
    "def generate_basic_statistics(df):\n",
    "    \"\"\"\n",
    "    Generate descriptive statistics for the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataset to analyze\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (numerical_columns, categorical_columns)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"📈 STEP 3: BASIC STATISTICS AND SUMMARIES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'bool']).columns\n",
    "    \n",
    "    # Numerical columns statistics\n",
    "    if len(numerical_cols) > 0:\n",
    "        print(\"🔢 Numerical columns summary:\")\n",
    "        print(df[numerical_cols].describe().round(2))\n",
    "    \n",
    "    # Categorical columns analysis\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\n📝 Categorical columns: {list(categorical_cols)}\")\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            print(f\"\\n📊 {col} - Value counts:\")\n",
    "            value_counts = df[col].value_counts().head(10)  # Show top 10 values\n",
    "            print(value_counts)\n",
    "            \n",
    "            if len(df[col].value_counts()) > 10:\n",
    "                print(f\"... and {len(df[col].value_counts()) - 10} more unique values\")\n",
    "    \n",
    "    print(f\"\\n📋 Summary:\")\n",
    "    print(f\"- Numerical columns: {len(numerical_cols)}\")\n",
    "    print(f\"- Categorical columns: {len(categorical_cols)}\")\n",
    "    print(f\"- Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    return numerical_cols, categorical_cols\n",
    "\n",
    "# CELL 8: Execute Basic Statistics\n",
    "# ================================\n",
    "\"\"\"\n",
    "### Let's analyze the statistical properties of our data:\n",
    "\"\"\"\n",
    "\n",
    "numerical_cols, categorical_cols = generate_basic_statistics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "351c3266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 STEP 4: DATA FILTERING AND SELECTION\n",
      "==================================================\n",
      "📋 1. Selecting specific columns (name and age):\n",
      "  first_name last_name  age\n",
      "0   Danielle   Johnson   58\n",
      "1    Roberto   Ramirez   65\n",
      "2    Melissa  Delacruz   19\n",
      "3     Jeremy   Johnson   59\n",
      "4     Daniel    Burton   66\n",
      "Selected 3 columns from original 17\n",
      "\n",
      "🔍 2. Filtering: Customers older than 50:\n",
      "Found 460 customers older than 50 (out of 1000 total)\n",
      "  first_name last_name  age\n",
      "0   Danielle   Johnson   58\n",
      "1    Roberto   Ramirez   65\n",
      "3     Jeremy   Johnson   59\n",
      "4     Daniel    Burton   66\n",
      "6     Andrea     Baker   72\n",
      "\n",
      "💰 3. Filtering: Active customers with high spending (>$2000):\n",
      "Found 303 high-value active customers\n",
      "   first_name last_name  total_spent  is_active\n",
      "0    Danielle   Johnson      3707.75       True\n",
      "7     Michele     Jones      2760.20       True\n",
      "11  Elizabeth   Edwards      2311.30       True\n",
      "12    Vanessa    Valdez      3880.00       True\n",
      "14       John    Garcia      2294.26       True\n",
      "\n",
      "💼 4. Filtering: Customers from IT department:\n",
      "Found 149 IT department customers\n",
      "   first_name last_name department                          job_title\n",
      "1     Roberto   Ramirez         IT                       Estate agent\n",
      "3      Jeremy   Johnson         IT  Research officer, political party\n",
      "23      Holly      Wang         IT                        Illustrator\n",
      "24      David     Riggs         IT    Geophysicist/field seismologist\n",
      "29   Kimberly     James         IT             Amenity horticulturist\n",
      "\n",
      "🏆 5. Filtering: Premium customers (Gold or Platinum):\n",
      "Found 493 premium customers\n",
      "customer_segment\n",
      "Platinum    252\n",
      "Gold        241\n",
      "Name: count, dtype: int64\n",
      "\n",
      "📧 6. Filtering: Customers with gmail addresses:\n",
      "Found 0 customers with Gmail addresses\n"
     ]
    }
   ],
   "source": [
    "# CELL 9: Data Filtering - Function Definition\n",
    "# ============================================\n",
    "\"\"\"\n",
    "## 🔍 Step 4: Data Filtering and Selection\n",
    "\n",
    "Filtering is one of the most important skills in data analysis:\n",
    "- **Column selection**: Choose specific columns for analysis\n",
    "- **Row filtering**: Find records that meet certain conditions\n",
    "- **Multiple conditions**: Combine filters using AND (&) and OR (|) operators\n",
    "- **Text filtering**: Search for specific text patterns\n",
    "\n",
    "This allows you to focus on relevant subsets of your data.\n",
    "\"\"\"\n",
    "\n",
    "def demonstrate_filtering(df):\n",
    "    \"\"\"\n",
    "    Show various ways to filter and select data\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataset to filter\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (filtered_dataframes...)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔍 STEP 4: DATA FILTERING AND SELECTION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Select specific columns\n",
    "    print(\"📋 1. Selecting specific columns (name and age):\")\n",
    "    basic_info = df[['first_name', 'last_name', 'age']].head()\n",
    "    print(basic_info)\n",
    "    print(f\"Selected {len(basic_info.columns)} columns from original {len(df.columns)}\")\n",
    "    \n",
    "    # 2. Filter rows based on single condition\n",
    "    print(\"\\n🔍 2. Filtering: Customers older than 50:\")\n",
    "    older_customers = df[df['age'] > 50]\n",
    "    print(f\"Found {len(older_customers)} customers older than 50 (out of {len(df)} total)\")\n",
    "    print(older_customers[['first_name', 'last_name', 'age']].head())\n",
    "    \n",
    "    # 3. Multiple conditions with AND\n",
    "    print(\"\\n💰 3. Filtering: Active customers with high spending (>$2000):\")\n",
    "    high_value_active = df[(df['is_active'] == True) & (df['total_spent'] > 2000)]\n",
    "    print(f\"Found {len(high_value_active)} high-value active customers\")\n",
    "    if len(high_value_active) > 0:\n",
    "        print(high_value_active[['first_name', 'last_name', 'total_spent', 'is_active']].head())\n",
    "    \n",
    "    # 4. Filter by categorical values\n",
    "    print(\"\\n💼 4. Filtering: Customers from IT department:\")\n",
    "    it_customers = df[df['department'] == 'IT']\n",
    "    print(f\"Found {len(it_customers)} IT department customers\")\n",
    "    if len(it_customers) > 0:\n",
    "        print(it_customers[['first_name', 'last_name', 'department', 'job_title']].head())\n",
    "    \n",
    "    # 5. Filter using isin() for multiple values\n",
    "    print(\"\\n🏆 5. Filtering: Premium customers (Gold or Platinum):\")\n",
    "    premium_customers = df[df['customer_segment'].isin(['Gold', 'Platinum'])]\n",
    "    print(f\"Found {len(premium_customers)} premium customers\")\n",
    "    print(premium_customers['customer_segment'].value_counts())\n",
    "    \n",
    "    # 6. String filtering (contains)\n",
    "    print(\"\\n📧 6. Filtering: Customers with gmail addresses:\")\n",
    "    gmail_customers = df[df['email'].str.contains('@gmail.com', na=False)]\n",
    "    print(f\"Found {len(gmail_customers)} customers with Gmail addresses\")\n",
    "    \n",
    "    return older_customers, high_value_active, it_customers, premium_customers\n",
    "\n",
    "# CELL 10: Execute Data Filtering\n",
    "# ===============================\n",
    "\"\"\"\n",
    "### Let's explore different filtering techniques:\n",
    "\"\"\"\n",
    "\n",
    "older_customers, high_value_active, it_customers, premium_customers = demonstrate_filtering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CELL 11: Data Grouping and Aggregation - Function Definition\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "## 📊 Step 5: Data Grouping and Aggregation\n",
    "\n",
    "Grouping allows you to analyze data by categories:\n",
    "- **Single grouping**: Group by one column (e.g., by department)\n",
    "- **Multiple grouping**: Group by several columns (e.g., by department and segment)\n",
    "- **Aggregation functions**: Calculate statistics for each group (mean, sum, count, etc.)\n",
    "- **Multiple aggregations**: Calculate different statistics for different columns\n",
    "\n",
    "This is essential for creating summaries and understanding patterns across different categories.\n",
    "\"\"\"\n",
    "\n",
    "def demonstrate_grouping(df):\n",
    "    \"\"\"\n",
    "    Show how to group data and perform aggregations\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataset to group\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (grouped_results...)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"📊 STEP 5: DATA GROUPING AND AGGREGATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Group by single column - simple aggregation\n",
    "    print(\"💰 1. Average spending by customer segment:\")\n",
    "    segment_spending = df.groupby('customer_segment')['total_spent'].mean().round(2)\n",
    "    print(segment_spending.sort_values(ascending=False))\n",
    "    \n",
    "    # 2. Group by single column - multiple statistics\n",
    "    print(\"\\n📊 2. Detailed statistics by customer segment:\")\n",
    "    segment_stats = df.groupby('customer_segment')['total_spent'].agg(['count', 'mean', 'median', 'std']).round(2)\n",
    "    segment_stats.columns = ['Customer Count', 'Avg Spending', 'Median Spending', 'Std Deviation']\n",
    "    print(segment_stats)\n",
    "    \n",
    "    # 3. Group by multiple columns\n",
    "    print(\"\\n🏢 3. Average salary by department and customer segment:\")\n",
    "    dept_segment_salary = df.groupby(['department', 'customer_segment'])['salary'].mean().round(2)\n",
    "    print(dept_segment_salary.head(15))\n",
    "    \n",
    "    # 4. Multiple aggregations on different columns\n",
    "    print(\"\\n📈 4. Comprehensive statistics by department:\")\n",
    "    dept_stats = df.groupby('department').agg({\n",
    "        'salary': ['mean', 'median', 'min', 'max'],\n",
    "        'age': 'mean',\n",
    "        'total_spent': ['sum', 'mean'],\n",
    "        'customer_id': 'count',\n",
    "        'is_active': lambda x: (x == True).sum()  # Count of active customers\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names for better readability\n",
    "    dept_stats.columns = ['Avg_Salary', 'Med_Salary', 'Min_Salary', 'Max_Salary', \n",
    "                         'Avg_Age', 'Total_Revenue', 'Avg_Spending', 'Customer_Count', 'Active_Count']\n",
    "    print(dept_stats)\n",
    "    \n",
    "    # 5. Percentage calculations\n",
    "    print(\"\\n📊 5. Active customer percentage by segment:\")\n",
    "    active_pct = df.groupby('customer_segment').apply(\n",
    "        lambda x: (x['is_active'].sum() / len(x) * 100).round(1)\n",
    "    ).sort_values(ascending=False)\n",
    "    print(active_pct)\n",
    "    \n",
    "    return segment_spending, dept_stats, active_pct\n",
    "\n",
    "# CELL 12: Execute Grouping and Aggregation\n",
    "# =========================================\n",
    "\"\"\"\n",
    "### Let's group our data and calculate meaningful statistics:\n",
    "\"\"\"\n",
    "\n",
    "segment_spending, dept_stats, active_pct = demonstrate_grouping(df)\n",
    "\n",
    "# CELL 13: Data Transformation - Function Definition\n",
    "# ==================================================\n",
    "\"\"\"\n",
    "## 🔄 Step 6: Data Transformation and Feature Engineering\n",
    "\n",
    "Creating new columns and transforming existing data:\n",
    "- **Calculated columns**: Create new columns from existing ones\n",
    "- **Categorical binning**: Convert continuous variables into categories\n",
    "- **Text manipulation**: Combine or split text columns\n",
    "- **Conditional columns**: Create columns based on conditions\n",
    "- **Mathematical operations**: Apply formulas to create derived metrics\n",
    "\n",
    "This helps create more meaningful insights and prepare data for analysis.\n",
    "\"\"\"\n",
    "\n",
    "def demonstrate_transformation(df):\n",
    "    \"\"\"\n",
    "    Show how to create new columns and transform data\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataset to transform\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Transformed dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔄 STEP 6: DATA TRANSFORMATION AND FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a copy to avoid modifying original data\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    # 1. Simple column combination\n",
    "    print(\"👥 1. Creating full name from first and last name:\")\n",
    "    df_transformed['full_name'] = df_transformed['first_name'] + ' ' + df_transformed['last_name']\n",
    "    print(\"✅ Created 'full_name' column\")\n",
    "    print(df_transformed['full_name'].head())\n",
    "    \n",
    "    # 2. Mathematical calculations\n",
    "    print(\"\\n💰 2. Calculating spending per order:\")\n",
    "    # Add 1 to avoid division by zero\n",
    "    df_transformed['spending_per_order'] = (df_transformed['total_spent'] / \n",
    "                                          (df_transformed['orders_count'] + 1)).round(2)\n",
    "    print(\"✅ Created 'spending_per_order' column\")\n",
    "    print(f\"Average spending per order: ${df_transformed['spending_per_order'].mean():.2f}\")\n",
    "    \n",
    "    # 3. Categorical binning - Age categories\n",
    "    print(\"\\n👴 3. Creating age categories:\")\n",
    "    def categorize_age(age):\n",
    "        if age < 30:\n",
    "            return 'Young (18-29)'\n",
    "        elif age < 50:\n",
    "            return 'Middle-aged (30-49)'\n",
    "        else:\n",
    "            return 'Senior (50+)'\n",
    "    \n",
    "    df_transformed['age_category'] = df_transformed['age'].apply(categorize_age)\n",
    "    print(\"✅ Created 'age_category' column\")\n",
    "    print(df_transformed['age_category'].value_counts())\n",
    "    \n",
    "    # 4. Binary indicator columns\n",
    "    print(\"\\n💎 4. Creating binary indicator columns:\")\n",
    "    median_spending = df_transformed['total_spent'].median()\n",
    "    df_transformed['high_spender'] = df_transformed['total_spent'] > median_spending\n",
    "    df_transformed['high_earner'] = df_transformed['salary'] > df_transformed['salary'].median()\n",
    "    print(f\"✅ Created binary columns based on median values\")\n",
    "    print(f\"High spenders (>${median_spending:.2f}+): {df_transformed['high_spender'].sum()}\")\n",
    "    \n",
    "    # 5. Date calculations (if date columns exist)\n",
    "    if 'registration_date' in df_transformed.columns:\n",
    "        print(\"\\n📅 5. Working with dates:\")\n",
    "        df_transformed['registration_date'] = pd.to_datetime(df_transformed['registration_date'])\n",
    "        df_transformed['days_since_registration'] = (pd.Timestamp.now() - df_transformed['registration_date']).dt.days\n",
    "        print(\"✅ Created 'days_since_registration' column\")\n",
    "        print(f\"Average customer tenure: {df_transformed['days_since_registration'].mean():.0f} days\")\n",
    "    \n",
    "    # 6. Text manipulation\n",
    "    print(\"\\n📧 6. Email domain extraction:\")\n",
    "    df_transformed['email_domain'] = df_transformed['email'].str.split('@').str[1]\n",
    "    print(\"✅ Created 'email_domain' column\")\n",
    "    print(\"Top email domains:\")\n",
    "    print(df_transformed['email_domain'].value_counts().head())\n",
    "    \n",
    "    # Display summary of new columns\n",
    "    new_columns = ['full_name', 'spending_per_order', 'age_category', 'high_spender', 'email_domain']\n",
    "    if 'days_since_registration' in df_transformed.columns:\n",
    "        new_columns.append('days_since_registration')\n",
    "    \n",
    "    print(f\"\\n📋 Summary: Created {len(new_columns)} new columns:\")\n",
    "    for col in new_columns:\n",
    "        if col in df_transformed.columns:\n",
    "            print(f\"- {col}\")\n",
    "    \n",
    "    print(f\"\\nDataset now has {len(df_transformed.columns)} columns (was {len(df.columns)})\")\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "# CELL 14: Execute Data Transformation\n",
    "# ===================================\n",
    "\"\"\"\n",
    "### Let's create new meaningful columns from our existing data:\n",
    "\"\"\"\n",
    "\n",
    "df_transformed = demonstrate_transformation(df)\n",
    "\n",
    "# CELL 15: Preview Transformed Data\n",
    "# ================================\n",
    "\"\"\"\n",
    "### Let's see our transformed data with new columns:\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔍 TRANSFORMED DATA PREVIEW\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Show some of the new columns\n",
    "preview_columns = ['full_name', 'age', 'age_category', 'total_spent', 'spending_per_order', 'high_spender']\n",
    "available_columns = [col for col in preview_columns if col in df_transformed.columns]\n",
    "\n",
    "print(\"Sample of transformed data:\")\n",
    "print(df_transformed[available_columns].head(10))\n",
    "\n",
    "# CELL 16: Sorting and Ranking - Function Definition\n",
    "# ==================================================\n",
    "\"\"\"\n",
    "## 📋 Step 7: Sorting and Ranking Data\n",
    "\n",
    "Organizing data in meaningful order:\n",
    "- **Single column sorting**: Order by one criterion\n",
    "- **Multiple column sorting**: Order by multiple criteria with different priorities\n",
    "- **Top/Bottom N**: Find the highest or lowest values\n",
    "- **Ranking**: Assign ranks to data points\n",
    "\n",
    "This helps identify patterns, outliers, and top performers in your data.\n",
    "\"\"\"\n",
    "\n",
    "def demonstrate_sorting(df):\n",
    "    \"\"\"\n",
    "    Show different ways to sort data\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataset to sort\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (sorted_results...)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"📋 STEP 7: SORTING AND RANKING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Sort by single column - highest values\n",
    "    print(\"💰 1. Top 10 highest spenders:\")\n",
    "    top_spenders = df.nlargest(10, 'total_spent')[['first_name', 'last_name', 'total_spent', 'customer_segment']]\n",
    "    print(top_spenders)\n",
    "    \n",
    "    # 2. Sort by single column - lowest values\n",
    "    print(\"\\n💸 2. Bottom 5 spenders:\")\n",
    "    low_spenders = df.nsmallest(5, 'total_spent')[['first_name', 'last_name', 'total_spent']]\n",
    "    print(low_spenders)\n",
    "    \n",
    "    # 3. Sort by multiple columns\n",
    "    print(\"\\n🏢 3. Sorted by department, then by salary (descending):\")\n",
    "    sorted_data = df.sort_values(['department', 'salary'], ascending=[True, False])\n",
    "    display_cols = ['first_name', 'last_name', 'department', 'salary', 'customer_segment']\n",
    "    print(sorted_data[display_cols].head(15))\n",
    "    \n",
    "    # 4. Add ranking columns\n",
    "    print(\"\\n🏆 4. Creating ranking columns:\")\n",
    "    df_ranked = df.copy()\n",
    "    df_ranked['spending_rank'] = df_ranked['total_spent'].rank(ascending=False, method='dense')\n",
    "    df_ranked['salary_rank'] = df_ranked['salary'].rank(ascending=False, method='dense')\n",
    "    \n",
    "    # Show top 10 by spending rank\n",
    "    top_ranked = df_ranked.nsmallest(10, 'spending_rank')[\n",
    "        ['first_name', 'last_name', 'total_spent', 'spending_rank', 'customer_segment']\n",
    "    ]\n",
    "    print(top_ranked)\n",
    "    \n",
    "    # 5. Sort by custom criteria\n",
    "    print(\"\\n⭐ 5. Best customers (high spending + high orders):\")\n",
    "    df['customer_score'] = (df['total_spent'] * 0.7) + (df['orders_count'] * 100 * 0.3)\n",
    "    best_customers = df.nlargest(10, 'customer_score')[\n",
    "        ['first_name', 'last_name', 'total_spent', 'orders_count', 'customer_score']\n",
    "    ]\n",
    "    print(best_customers.round(2))\n",
    "    \n",
    "    return top_spenders, best_customers\n",
    "\n",
    "# CELL 17: Execute Sorting and Ranking\n",
    "# ====================================\n",
    "\"\"\"\n",
    "### Let's sort our data to find patterns and top performers:\n",
    "\"\"\"\n",
    "\n",
    "top_spenders, best_customers = demonstrate_sorting(df_transformed)\n",
    "\n",
    "# CELL 18: Data Export - Function Definition\n",
    "# ==========================================\n",
    "\"\"\"\n",
    "## 💾 Step 8: Saving and Exporting Data\n",
    "\n",
    "After analyzing and transforming data, you'll want to save your results:\n",
    "- **Full dataset**: Save the complete transformed dataset\n",
    "- **Filtered subsets**: Save specific filtered data\n",
    "- **Summary reports**: Export aggregated results\n",
    "- **Different formats**: CSV, Excel, or other formats\n",
    "\n",
    "This preserves your work and allows sharing results with others.\n",
    "\"\"\"\n",
    "\n",
    "def save_processed_data(df, df_transformed, filename_prefix='processed'):\n",
    "    \"\"\"\n",
    "    Save processed data to new CSV files\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Original dataset\n",
    "    df_transformed (pandas.DataFrame): Transformed dataset\n",
    "    filename_prefix (str): Prefix for output files\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"💾 STEP 8: SAVING AND EXPORTING DATA\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Save the complete transformed dataset\n",
    "    transformed_filename = f'{filename_prefix}_complete_data.csv'\n",
    "    df_transformed.to_csv(transformed_filename, index=False)\n",
    "    print(f\"✅ Complete transformed data saved to: {transformed_filename}\")\n",
    "    print(f\"   - Rows: {len(df_transformed)}, Columns: {len(df_transformed.columns)}\")\n",
    "    \n",
    "    # 2. Save a customer summary report\n",
    "    summary_filename = f'{filename_prefix}_customer_summary.csv'\n",
    "    summary_columns = ['customer_id', 'full_name', 'age', 'age_category', \n",
    "                      'total_spent', 'customer_segment', 'department', 'is_active']\n",
    "    available_summary_cols = [col for col in summary_columns if col in df_transformed.columns]\n",
    "    \n",
    "    df_transformed[available_summary_cols].to_csv(summary_filename, index=False)\n",
    "    print(f\"✅ Customer summary saved to: {summary_filename}\")\n",
    "    print(f\"   - Rows: {len(df_transformed)}, Columns: {len(available_summary_cols)}\")\n",
    "    \n",
    "    # 3. Save high-value customers only\n",
    "    if 'high_spender' in df_transformed.columns:\n",
    "        high_value_filename = f'{filename_prefix}_high_value_customers.csv'\n",
    "        high_value_customers = df_transformed[df_transformed['high_spender'] == True]\n",
    "        high_value_customers.to_csv(high_value_filename, index=False)\n",
    "        print(f\"✅ High-value customers saved to: {high_value_filename}\")\n",
    "        print(f\"   - Rows: {len(high_value_customers)}, Columns: {len(high_value_customers.columns)}\")\n",
    "    \n",
    "    # 4. Save department analysis\n",
    "    dept_analysis_filename = f'{filename_prefix}_department_analysis.csv'\n",
    "    dept_analysis = df_transformed.groupby('department').agg({\n",
    "        'customer_id': 'count',\n",
    "        'salary': 'mean',\n",
    "        'total_spent': 'mean',\n",
    "        'age': 'mean',\n",
    "        'is_active': lambda x: (x == True).sum()\n",
    "    }).round(2)\n",
    "    dept_analysis.columns = ['Employee_Count', 'Avg_Salary', 'Avg_Spending', 'Avg_Age', 'Active_Count']\n",
    "    dept_analysis.to_csv(dept_analysis_filename)\n",
    "    print(f\"✅ Department analysis saved to: {dept_analysis_filename}\")\n",
    "    \n",
    "    print(f\"\\n📁 All files saved successfully!\")\n",
    "    return transformed_filename, summary_filename\n",
    "\n",
    "# CELL 19: Execute Data Export\n",
    "# ============================\n",
    "\"\"\"\n",
    "### Let's save our processed data and analysis results:\n",
    "\"\"\"\n",
    "\n",
    "transformed_file, summary_file = save_processed_data(df, df_transformed)\n",
    "\n",
    "# CELL 20: Advanced Analysis Examples\n",
    "# ===================================\n",
    "\"\"\"\n",
    "## 🎓 Step 9: Advanced Analysis Examples\n",
    "\n",
    "Now that you've learned the basics, let's explore some advanced techniques:\n",
    "\"\"\"\n",
    "\n",
    "print(\"🎓 ADVANCED ANALYSIS EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Correlation Analysis\n",
    "print(\"📊 1. Correlation analysis between numerical variables:\")\n",
    "numerical_data = df_transformed.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numerical_data.corr().round(3)\n",
    "\n",
    "# Show correlations with total_spent\n",
    "if 'total_spent' in correlation_matrix.columns:\n",
    "    spending_correlations = correlation_matrix['total_spent'].sort_values(ascending=False)\n",
    "    print(\"Variables correlated with total spending:\")\n",
    "    print(spending_correlations)\n",
    "\n",
    "# 2. Cross-tabulation\n",
    "print(\"\\n📋 2. Cross-tabulation: Customer segment vs Department:\")\n",
    "if 'age_category' in df_transformed.columns:\n",
    "    cross_tab = pd.crosstab(df_transformed['customer_segment'], \n",
    "                           df_transformed['age_category'], \n",
    "                           margins=True)\n",
    "    print(cross_tab)\n",
    "\n",
    "# 3. Percentile analysis\n",
    "print(\"\\n📈 3. Spending percentiles by customer segment:\")\n",
    "spending_percentiles = df_transformed.groupby('customer_segment')['total_spent'].quantile([0.25, 0.5, 0.75]).round(2)\n",
    "print(spending_percentiles)\n",
    "\n",
    "# CELL 21: Data Visualization Examples\n",
    "# ====================================\n",
    "\"\"\"\n",
    "## 📊 Step 10: Basic Data Visualization\n",
    "\n",
    "Let's create some simple visualizations to better understand our data:\n",
    "\"\"\"\n",
    "\n",
    "print(\"📊 CREATING DATA VISUALIZATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Distribution of total spending\n",
    "axes[0, 0].hist(df_transformed['total_spent'], bins=30, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribution of Total Spending')\n",
    "axes[0, 0].set_xlabel('Total Spent ($)')\n",
    "axes[0, 0].set_ylabel('Number of Customers')\n",
    "\n",
    "# 2. Average spending by customer segment\n",
    "segment_avg = df_transformed.groupby('customer_segment')['total_spent'].mean()\n",
    "axes[0, 1].bar(segment_avg.index, segment_avg.values, color=['bronze', 'silver', 'gold', 'purple'])\n",
    "axes[0, 1].set_title('Average Spending by Customer Segment')\n",
    "axes[0, 1].set_ylabel('Average Spending ($)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Age distribution\n",
    "axes[1, 0].hist(df_transformed['age'], bins=20, alpha=0.7, color='lightgreen')\n",
    "axes[1, 0].set_title('Age Distribution')\n",
    "axes[1, 0].set_xlabel('Age')\n",
    "axes[1, 0].set_ylabel('Number of Customers')\n",
    "\n",
    "# 4. Customer count by department\n",
    "dept_counts = df_transformed['department'].value_counts()\n",
    "axes[1, 1].pie(dept_counts.values, labels=dept_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 1].set_title('Customer Distribution by Department')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Visualizations created successfully!\")\n",
    "\n",
    "# CELL 22: Summary and Next Steps\n",
    "# ===============================\n",
    "\"\"\"\n",
    "# 🎉 Congratulations! Tutorial Complete!\n",
    "\n",
    "## What you've accomplished:\n",
    "✅ **Data Loading**: Loaded and explored CSV data  \n",
    "✅ **Quality Assessment**: Checked for missing values and duplicates  \n",
    "✅ **Statistical Analysis**: Generated descriptive statistics  \n",
    "✅ **Data Filtering**: Selected and filtered data based on conditions  \n",
    "✅ **Grouping & Aggregation**: Summarized data by categories  \n",
    "✅ **Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc06e20d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
